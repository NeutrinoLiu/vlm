{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from templates.filter import *\n",
    "from templates.func import *\n",
    "from templates.QA import QADataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"scannetpp\"\n",
    "os.chdir(f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/datasets/{DATASET}\")\n",
    "\n",
    "DS_ROOT = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/datasets/{DATASET}/structured-data\"\n",
    "CAP_FILE = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/datasets/{DATASET}/captions_all.yaml\"\n",
    "\n",
    "N_TRAIN = 100_000\n",
    "N_TEST = 2_000\n",
    "\n",
    "def scene_valid_fn(sc_name):\n",
    "    scene_path = os.path.join(DS_ROOT, sc_name)\n",
    "    finished_flag = \"finished.flag\"\n",
    "    finished_flag_path = os.path.join(scene_path, finished_flag)\n",
    "    return os.path.exists(finished_flag_path)\n",
    "\n",
    "random.seed(0)\n",
    "myCap = Captioner(CAP_FILE) if os.path.exists(CAP_FILE) else None\n",
    "ds = QADataset(DS_ROOT, myCap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img/video related arguments\n",
    "grid_cfg_file = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/QA/grid_cfg_{DATASET}.json\"\n",
    "with open(grid_cfg_file, \"r\") as f:\n",
    "    grid_cfg = json.load(f)\n",
    "H = grid_cfg[\"H\"]\n",
    "W = grid_cfg[\"W\"]\n",
    "patchsize_H = grid_cfg[\"patchsize_H\"]\n",
    "patchsize_W = grid_cfg[\"patchsize_W\"]\n",
    "n_frames = 6\n",
    "frame_stride = 4\n",
    "\n",
    "\n",
    "tasks_cfg = {\n",
    "    \"total_QAs\": 100000,\n",
    "    \"roi_frame_only\": True,\n",
    "    \"H\": H, \"W\": W, \"patchsize_H\": patchsize_H, \"patchsize_W\": patchsize_W,\n",
    "    \"motion_thres\": 4, \"num_frame\": n_frames, \"frame_stride\": frame_stride,\n",
    "    \"prefix\": \"scannetpp_new\"\n",
    "}\n",
    "\n",
    "\n",
    "total_qas = tasks_cfg[\"total_QAs\"]\n",
    "H_grids = tasks_cfg[\"H\"] // tasks_cfg[\"patchsize_H\"]\n",
    "W_grids = tasks_cfg[\"W\"] // tasks_cfg[\"patchsize_W\"]\n",
    "prefix = tasks_cfg[\"prefix\"]\n",
    "\n",
    "task_dir = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/workspace/image_{prefix}_{H_grids}x{W_grids}_{n_frames}\"\n",
    "OUTPUT_DIR = os.path.join(task_dir, \"pairs\")\n",
    "OUTPUT_QWEN = f\"{OUTPUT_DIR}/QA_pairs_qwen.json\"\n",
    "OUTPUT_JSON = f\"{OUTPUT_DIR}/QA_pairs.json\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "task_obj = {\n",
    "    os.path.basename(task_dir): {\n",
    "        \"train_qa\": OUTPUT_QWEN.replace(\".json\", \".train.json\"),\n",
    "        \"test_qa\": OUTPUT_QWEN.replace(\".json\", \".test.json\"),\n",
    "        \"train_qa_meta\": OUTPUT_JSON.replace(\".json\", \".train.json\"),\n",
    "        \"test_qa_meta\": OUTPUT_JSON.replace(\".json\", \".test.json\"),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(task_obj, indent=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
