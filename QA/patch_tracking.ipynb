{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcdb7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70050de6172e47929dffc0241fdee808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# should move to dataset path\n",
    "dataset = \"scannetpp\"\n",
    "os.chdir(f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/datasets/{dataset}\")\n",
    "DS_ROOT = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/datasets/{dataset}/structured-data\"\n",
    "TEST_SPLIT = 0.2\n",
    "CAP_FILE = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/datasets/{dataset}/captions_all.yaml\"\n",
    "grid_cfg_file = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/QA/grid_cfg_{dataset}.json\"\n",
    "\n",
    "# function pool here\n",
    "from templates.filter import *\n",
    "from templates.func import *\n",
    "from templates.QA import QADataset\n",
    "\n",
    "random.seed(0)\n",
    "myCap = Captioner(CAP_FILE)\n",
    "ds = QADataset(DS_ROOT, myCap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970cc65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{\n",
      "    \"image_nohint_16x24_2\": {\n",
      "        \"train_qa\": \"/mnt/bn/nlhei-nas/liubangya/proj/vlm/workspace/image_nohint_16x24_2/pairs/QA_pairs_qwen.train.json\",\n",
      "        \"test_qa\": \"/mnt/bn/nlhei-nas/liubangya/proj/vlm/workspace/image_nohint_16x24_2/pairs/QA_pairs_qwen.test.json\",\n",
      "        \"train_qa_meta\": \"/mnt/bn/nlhei-nas/liubangya/proj/vlm/workspace/image_nohint_16x24_2/pairs/QA_pairs.train.json\",\n",
      "        \"test_qa_meta\": \"/mnt/bn/nlhei-nas/liubangya/proj/vlm/workspace/image_nohint_16x24_2/pairs/QA_pairs.test.json\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from templates.task.tracking_tasks import TrackingTasks\n",
    "\n",
    "\n",
    "with open(grid_cfg_file, \"r\") as f:\n",
    "    grid_cfg = json.load(f)\n",
    "H = grid_cfg[\"H\"]\n",
    "W = grid_cfg[\"W\"]\n",
    "patchsize_H = grid_cfg[\"patchsize_H\"]\n",
    "patchsize_W = grid_cfg[\"patchsize_W\"]\n",
    "n_frames = 2\n",
    "\n",
    "tasks_cfg = {\n",
    "    \"total_QAs\": 4000,\n",
    "    \"roi_frame_only\": True,\n",
    "    \"H\": H, \"W\": W, \"patchsize_H\": patchsize_H, \"patchsize_W\": patchsize_W,\n",
    "    \"motion_thres\": n_frames, \"num_frame\": n_frames\n",
    "}\n",
    "\n",
    "\n",
    "total_qas = tasks_cfg[\"total_QAs\"]\n",
    "H_grids = tasks_cfg[\"H\"] // tasks_cfg[\"patchsize_H\"]\n",
    "W_grids = tasks_cfg[\"W\"] // tasks_cfg[\"patchsize_W\"]\n",
    "# task_dir = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm-found3d/tasks/grid_idx_{H_grids}x{W_grids}\"\n",
    "task_dir = f\"/mnt/bn/nlhei-nas/liubangya/proj/vlm/workspace/image_nohint_{H_grids}x{W_grids}_{n_frames}\"\n",
    "OUTPUT_DIR = os.path.join(task_dir, \"pairs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_QWEN = f\"{OUTPUT_DIR}/QA_pairs_qwen.json\"\n",
    "OUTPUT_JSON = f\"{OUTPUT_DIR}/QA_pairs.json\"\n",
    "\n",
    "task_obj = {\n",
    "    os.path.basename(task_dir): {\n",
    "        \"train_qa\": OUTPUT_QWEN.replace(\".json\", \".train.json\"),\n",
    "        \"test_qa\": OUTPUT_QWEN.replace(\".json\", \".test.json\"),\n",
    "        \"train_qa_meta\": OUTPUT_JSON.replace(\".json\", \".train.json\"),\n",
    "        \"test_qa_meta\": OUTPUT_JSON.replace(\".json\", \".test.json\"),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(task_obj, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 QAs, stats: {'obj_cross_frame_tracking': 500}\n",
      "Generated 1000 QAs, stats: {'obj_cross_frame_tracking': 1000}\n",
      "Warning: 100 duplicated hashes\n",
      "Generated 1500 QAs, stats: {'obj_cross_frame_tracking': 1500}\n",
      "Generated 2000 QAs, stats: {'obj_cross_frame_tracking': 2000}\n",
      "Warning: 200 duplicated hashes\n",
      "Generated 2500 QAs, stats: {'obj_cross_frame_tracking': 2500}\n",
      "Warning: 300 duplicated hashes\n",
      "Generated 3000 QAs, stats: {'obj_cross_frame_tracking': 3000}\n",
      "Warning: 400 duplicated hashes\n",
      "Warning: 500 duplicated hashes\n",
      "Generated 3500 QAs, stats: {'obj_cross_frame_tracking': 3500}\n",
      "Warning: 600 duplicated hashes\n",
      "Warning: 700 duplicated hashes\n",
      "Generated 4000 QAs, stats: {'obj_cross_frame_tracking': 4000}\n",
      "total 4000 qas\n",
      "stats: {\n",
      "  \"obj_cross_frame_tracking\": 4000\n",
      "}\n",
      "total objects: 1301\n",
      "total scenes: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "myfilter = filter_all(\n",
    "    filter_visiblity,\n",
    "    filter_area_fn(1e4, 4e5),\n",
    "    black_list_fn([\n",
    "            \"movable_object.trafficcone\",\n",
    "            \"movable_object.barrier\",\n",
    "        ])\n",
    "    )\n",
    "\n",
    "\n",
    "taskset = TrackingTasks(\n",
    "    captioner=myCap,\n",
    "    basefilter=myfilter,\n",
    "    cfg=tasks_cfg\n",
    ")\n",
    "\n",
    "\n",
    "qas, stats = taskset.produce(\n",
    "    dataset=ds,\n",
    "    num_qas=total_qas,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"total {len(qas)} qas\")\n",
    "print(f\"stats: {json.dumps(stats, indent=2)}\")\n",
    "\n",
    "all_dumps = [qa.dump() for qa in qas]\n",
    "content_stats = {\n",
    "    \"objs\": set(),\n",
    "    \"scenes\": set(),\n",
    "}\n",
    "for qa in all_dumps:\n",
    "    content_stats[\"objs\"].update(qa[\"objs\"])\n",
    "    content_stats[\"scenes\"].update([qa[\"scene\"]])\n",
    "print(f\"total objects: {len(content_stats['objs'])}\")\n",
    "print(f\"total scenes: {len(content_stats['scenes'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3af082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# adding idx\n",
    "# process image idx\n",
    "\n",
    "num_test = int(len(qas) * TEST_SPLIT)\n",
    "qas_train = qas[:-num_test]\n",
    "qas_test = qas[-num_test:]\n",
    "\n",
    "all_frames = not taskset.cfg[\"roi_frame_only\"]\n",
    "\n",
    "with open(OUTPUT_QWEN.replace(\".\", \".test.\"), \"w\") as f:\n",
    "    qas_dumps = [qa.qwen_format(all_frames=all_frames) for qa in qas_test]\n",
    "    for i, qa in enumerate(qas_dumps):\n",
    "        qa[\"id\"] = i\n",
    "    json.dump(\n",
    "        qas_dumps, f, indent=2\n",
    "    )\n",
    "with open(OUTPUT_QWEN.replace(\".\", \".train.\"), \"w\") as f:\n",
    "    qas_dumps = [qa.qwen_format(all_frames=all_frames) for qa in qas_train]\n",
    "    for i, qa in enumerate(qas_dumps):\n",
    "        qa[\"id\"] = i\n",
    "    json.dump(\n",
    "        qas_dumps, f, indent=2\n",
    "    )\n",
    "with open(OUTPUT_JSON.replace(\".\", \".test.\"), \"w\") as f:\n",
    "    qas_dumps = [qa.dump() for qa in qas_test]\n",
    "    for i, qa in enumerate(qas_dumps):\n",
    "        qa[\"id\"] = i\n",
    "    json.dump(qas_dumps, f, indent=2)\n",
    "with open(OUTPUT_JSON.replace(\".\", \".train.\"), \"w\") as f:\n",
    "    qas_dumps = [qa.dump() for qa in qas_train]\n",
    "    for i, qa in enumerate(qas_dumps):\n",
    "        qa[\"id\"] = i\n",
    "    json.dump(qas_dumps, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "fileId": "5e024d9d-3d83-4fc8-b52d-d080fd5172bb",
  "filePath": "/mnt/bn/nlhei-nas/liubangya/proj/vlm_proj/QA/templating-naive.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
